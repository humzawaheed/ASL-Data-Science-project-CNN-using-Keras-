{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing important module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "# from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Kaggle for importing its dataset using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Directory and moving kaggle.json file to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the ASL dataset from kaggle using there API command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zipfile import ZipFile\n",
    "# dataset = \"sign-language-mnist.zip\"\n",
    "# with ZipFile(dataset, \"r\") as zip:\n",
    "#     zip.extractall()\n",
    "#     print(\"data is extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can play with the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pandas to read CSV files of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "# test_df = pd.read_csv(\"sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Head command to view first five rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,8)) # set any size you want\n",
    "# sns.set_style(\"darkgrid\") # we can use white,dark,whitegrid,darkgrid,ticks etc as per our requirements\n",
    "# sns.countplot(data=train_df, x=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems balanced as for each training label , enough training examples exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_only_labels = train_df[\"label\"]\n",
    "# test_df_only_labels =  test_df[\"label\"]\n",
    "# del train_df[\"label\"]\n",
    "# del test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBinarizer = LabelBinarizer()\n",
    "# y_train = LabelBinarizer.fit_transform(train_df_only_labels) # this convert all the labels in binary form: (0s and 1s)\n",
    "# y_test = LabelBinarizer.fit_transform(test_df_only_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will assign all the values of the dataset except the \"labels\" to the new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = train_df.values\n",
    "# x_test = test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform a grayscale normalization to reduce the effect of illumination's differences.Moreover the CNN converges faster on [0..1] data than on [0..255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalizing the data\n",
    "# x_train = x_train/255\n",
    "# x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the data from 1-D to 3-D as required through input by CNN's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "# x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of first ten images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax=plt.subplots(2, 5)\n",
    "# f.set_size_inches(10, 10)\n",
    "# k = 0\n",
    "# for i in range(2):\n",
    "#     for j in range(5):\n",
    "#         ax[i, j].imshow(x_train[k].reshape(28,28), cmap = \"gray\")\n",
    "#         k+=1\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Data Augmentation to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_data = ImageDataGenerator(\n",
    "#     featurewise_center= False,              # set input mean to 0 over the dataset\n",
    "#     samplewise_center= False,               # set each sample mean to 0\n",
    "#     featurewise_std_normalization= False,   # divide input by the std of the dataset\n",
    "#     samplewise_std_normalization= False,    # divide each output by its std\n",
    "#     zca_whitening= False,                   # apply ZCA whitening\n",
    "#     rotation_range= 10,                     # randomly rotate images in the range of (0,180) degree\n",
    "#     zoom_range= 0.1,                        # randomly zoom images\n",
    "#     width_shift_range= 0.1,                 # randomly shift images horizontally (fraction of total width)\n",
    "#     height_shift_range= 0.1,                # randomly shift images vertically (fraction of total height)\n",
    "#     horizontal_flip= False,                 # randomly flip images\n",
    "#     vertical_flip= False,                   # randomly flip images\n",
    "# )\n",
    "# augmented_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data augmentation, i choosed to :\n",
    "\n",
    "Randomly rotate some training images by 10 degrees Randomly Zoom by 10% some training images Randomly shift images horizontally by 10% of the width Randomly shift images vertically by 10% of the height I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify.\n",
    "\n",
    "Once our model is ready, we fit the training dataset ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORKS\n",
    "A Convolutional Neural Network(CNN) is a special type of an Artificial Intelligence implementation which uses a special mathematical matrix manipulation called the convolution operation to process data from the images.\n",
    "\n",
    "A convolution does this by multiplying two matrices and yielding a third, smaller matrix.\n",
    "The Network takes an input image, and uses a filter (or kernel) to create a feature map describing the image.\n",
    "In the convolution operation, we take a filter (usually 2x2 or 3x3 matrix ) and slide it over the image matrix. The coresponding numbers in both matrices are multiplied and and added to yield a single number describing that input space. This process is repeated all over the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor=\"val_accuracy\", patience=2, verbose=1, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(250, (3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "# model.add(Conv2D(225, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "# model.add(Conv2D(200, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(175, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(150, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(125, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(100, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(75, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(50, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Conv2D(25, (3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=512, activation=\"relu\"))\n",
    "# model.add(Dropout(0.6))\n",
    "# model.add(Dense(units=24, activation=\"relu\"))\n",
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", sample_weight_mode=[\"accuracy\"])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(augmented_data.flow(x_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (x_test, y_test) , callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humza\n"
     ]
    }
   ],
   "source": [
    "print(\"humza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
